{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\ntitle       : \"Extremum estimation\"\nsubtitle    :\nauthor      : Paul Schrimpf\ndate        : `j using Dates; print(Dates.today())`\nbibliography: \"ee.bib\"\n---\n\n<a rel=\"license\"\nhref=\"http://creativecommons.org/licenses/by-sa/4.0/\"><img alt=\"Creative\nCommons License\" style=\"border-width:0\"\nsrc=\"https://i.creativecommons.org/l/by-sa/4.0/88x31.png\"\n/></a><br />This work is licensed under a <a rel=\"license\"\nhref=\"http://creativecommons.org/licenses/by-sa/4.0/\">Creative\nCommons Attribution-ShareAlike 4.0 International License</a>.\n\n### About this document {-}\n\nThis document was created using Weave.jl. The code is available in\n[the course github\nrepository](https://github.com/ubcecon/ECON628_2018). The same\ndocument generates both static webpages and associated jupyter\nnotebooks.\n\n$$\n\\def\\indep{\\perp\\!\\!\\!\\perp}\n\\def\\Er{\\mathrm{E}}\n\\def\\R{\\mathbb{R}}\n\\def\\En{{\\mathbb{E}_n}}\n\\def\\Pr{\\mathrm{P}}\n\\newcommand{\\norm}[1]{\\left\\Vert {#1} \\right\\Vert}\n\\newcommand{\\abs}[1]{\\left\\vert {#1} \\right\\vert}\n\\DeclareMathOperator*{\\argmax}{arg\\,max}\n\\DeclareMathOperator*{\\argmin}{arg\\,min}\n\\def\\inprob{\\,{\\buildrel p \\over \\rightarrow}\\,} \n\\def\\indist{\\,{\\buildrel d \\over \\rightarrow}\\,} \n$$\n\n# Introduction\n\nAs you saw in 627, many, perhaps most, estimators in econometrics are\nextrumem estimators. That is, many estimators are defined by\n$$\n\\hat{\\theta} = \\argmax_{\\theta \\in \\Theta} \\hat{Q}_n(\\theta)\n$$\nwhere $\\hat{Q}_n(\\theta)$ is some objective function that depends on\ndata. Examples include maximum likelihood,\n$$\n\\hat{Q}_n(\\theta) = \\frac{1}{n} \\sum_{i=1}^n f(z_i | \\theta)\n$$\nGMM,\n$$\n\\hat{Q}_n(\\theta) = \\left(\\frac{1}{n} \\sum_{i=1}^n g(z_i,\n\\theta)\\right)' \\hat{W} \\left(\\frac{1}{n} \\sum_{i=1}^n g(z_i,\n\\theta)\\right)\n$$\nand nonlinear least squares\n$$\n\\hat{Q}_n(\\theta) = \\frac{1}{n} \\sum_{i=1}^n (y_i - h(x_i,\\theta))^2.\n$$\nSee @newey1994 for more details and examples.\n\n## Example: logit\n\nAs a simple example, let's look look at some code for estimating a\nlogit."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using Distributions, Optim, ForwardDiff, BenchmarkTools\nfunction simulate_logit(observations, β)\n  x = randn(observations, length(β))\n  y = (x*β + rand(Logistic(), observations)) .>= 0.0\n  return((y=y,x=x))\nend \n\nfunction logit_likelihood(β,y,x)\n  p = map(xb -> cdf(Logistic(),xb), x*β)\n  sum(log.(ifelse.(y, p, 1.0 .- p)))\nend\n\nn = 500\nk = 3\nβ0 = ones(k)\n(y,x) = simulate_logit(n,β0)\nQ = β -> -logit_likelihood(β,y,x)\nQ(β0)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "@btime optimize(Q, zeros(k), NelderMead())\n@btime optimize(Q, zeros(k), BFGS(), autodiff = :forward)\n@btime optimize(Q, zeros(k), NewtonTrustRegion(), autodiff =:forward)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Aside: Reverse mode automatic differentiation\n\nFor functions $f:\\R^n \\to \\R^m$, the work for forward\nautomatic differentiation increases linearly with $n$. This is because\nforward automatic differentiation applies the chain rule to each of the $n$\ninputs. An alternative, is reverse automatic differentiation. Reverse\nautomatic differentiation is also based on the chain rule, but it\nworks backward from $f$ through intermediate steps back to $x$. The\nwork needed here scales linearly with $m$. Since optimization problems\nhave $m=1$, reverse automatic differentiation can often work well. The\ndownsides of reverse automatic differentiation are that: (1) it can\nrequire a large amount of memory and (2) it is more difficult to\nimplement. There are handful of Julia packages that provide reverse\nautomatic differentiation, but they all have some limitations in terms\nof what functions thay can differentiate. Flux.jl and JuMP.jl, which were included\nas examples in the Solvers, Optimizers, and Automatic Differentiation\nnotebook, uses reverse automatic differentiation. ReverseDiff.jl is\nanother package for reverse automatic differentiation. ReverseDiff.jl\nplaces fewer restrictions on the functions that it can differentiate,\nbut its creators no longer plan to update it."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using ReverseDiff, Optim, BenchmarkTools\ndQr = β->ReverseDiff.gradient(Q,β)\ndQf = β->ForwardDiff.gradient(Q,β)\n\nhcat(dQr(β0) , dQf(β0))\n\n@btime optimize(Q, dQf, zeros(k), BFGS(); inplace=false)\n@btime optimize(Q, dQr, zeros(k), BFGS(); inplace=false)\n\nk = 100\nβ0 = ones(k)\n(y,x) = simulate_logit(n,β0)\nQ = β -> -logit_likelihood(β,y,x)\ndQr = β->ReverseDiff.gradient(Q,β)\ndQf = β->ForwardDiff.gradient(Q,β)\nhcat(dQr(β0) , dQf(β0))\n@btime optimize(Q, dQf, zeros(k), BFGS(); inplace=false)\n@btime optimize(Q, dQr, zeros(k), BFGS(); inplace=false)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Review of extremum estimator theory\n\nThis is based on @newey1994. You should already be familiar with this\nfrom 627, so we will just state some basic \"high-level\" conditions for\nconsistency and asymptotic normality. \n\n## Consistency\n\n<div class=\"theorem\">\n**Consistency for extremum estimators**: assume\n\n1. $\\hat{Q}_n(\\theta)$ converges uniformly in probability to\n   $Q_0(\\theta)$\n2. $Q_0(\\theta)$ is uniquely maximized at $\\theta_0$.\n3. $\\Theta$ is compact and $Q_0(\\theta)$ is continuous.\n\nThen $\\hat{\\theta} \\inprob \\theta_0$\n</div>\n\n\n## Asymptotic normality\n\n<div class=\"theorem\">\n**Asymptotic normality for extremum estimators**: assume\n1. $\\hat{\\theta} \\inprob \\theta_0$\n2. $\\theta_0 \\in interior(\\Theta)$\n3. $\\hat{Q}_n(\\theta)$ is twice continuously differentiable in open $N$\n   containing $\\theta$, and $\\sup_{\\theta \\in N} \\Vert \\nabla^2\n                             \\hat{Q}_n(\\theta) - H(\\theta) \\Vert\n                             \\inprob 0$ with $H(\\theta_0)$ nonsingular\n4. $\\sqrt{n} \\nabla \\hat{Q}_n(\\theta_0) \\indist N(0,\\Sigma)$\nThen $\\sqrt{n} (\\hat{\\theta} - \\theta_0) \\indist N\\left(0,H^{-1} \\Sigma\n  H^{-1} \\right)$\n</div>\n\nImplementing this in Julia using automatic differentiation is pretty\nstraightforward."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "function logit_likei(β,y,x)\n  p = map(xb -> cdf(Logistic(),xb), x*β)\n  log.(ifelse.(y, p, 1.0 .- p))\nend\n\nfunction logit_likelihood(β,y,x)\n  mean(logit_likei(β,y,x))\nend\n\nn = 1000\nk = 3\nβ0 = ones(k)\n(y,x) = simulate_logit(n,β0)\n      \nQ = β -> -logit_likelihood(β,y,x)      \noptres = optimize(Q, zeros(k), NewtonTrustRegion(), autodiff =:forward)\nβhat = optres.minimizer\n\nfunction asymptotic_variance(Q,dQi, θ)\n  gi = dQi(θ)\n  Σ = gi'*gi/size(gi)[1]\n  H = ForwardDiff.hessian(Q,θ)\n  invH = inv(H)\n  (variance=invH*Σ*invH, Σ=Σ, invH=invH)\nend\n\navar=asymptotic_variance(θ->logit_likelihood(θ,y,x),\n                         θ->ForwardDiff.jacobian(β->logit_likei(β,y,x),θ),βhat)\n@show avar.variance/n\n@show -avar.invH/n\n@show inv(avar.Σ)/n"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "For maximum likelihood, the information equality says $-H = \\Sigma$,\nso the three expressions above have the same probability limit, and\nare each consistent estimates of the variance of $\\hat{\\theta}$.\n\nThe code above is for demonstration and learning. If we really wanted\nto estimate a logit for research, it would be better to use a\nwell-tested package. Here's how to estimate  a logit using GLM.jl."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using GLM, DataFrames\ndf = DataFrame(x)\ndf[:y] = y\nglmest=glm(@formula(y ~ -1 + x1+x2+x3), df, Binomial(),LogitLink())\n@show glmest\n@show vcov(glmest)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Delta method\n\nIn many models, we are interested in some transformation of the\nparameters in addition to the parameters themselves. For example, in a\nlogit, we might want to report marginal effects in addition to the\ncoefficients. In structural models, we typically use the parameter\nestimates to conduct counterfactual simulations. In many \nsituations we are more interested these transformation(s) of\nparameters than in the parameters themselves. The delta method is one\nconvenient way to approximate the distribution of transformations of \nthe model parameters. \n\n<div class=\"theorem\">\n**Delta method** assume:\n1. $\\sqrt{n} (\\hat{\\theta} - \\theta_0) \\indist N(0,\\Omega)$\n2. $g: \\R^k \\to \\R^m$ is continuously differentiable\nThen $\\sqrt{n}(g(\\hat{\\theta}) - g(\\theta_0)) \\indist N(0, \\nabla\n      g(\\theta_0)^T \\Omega \\nabla g(\\theta_0)$\n</div>\n\nThe following code uses the delta method to plot a 90% pointwise\nconfidence band around the estimate marginal effect of one of the\nregressors."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using LinearAlgebra\nfunction logit_mfx(β,x)\n  ForwardDiff.jacobian(β-> map(xb -> cdf(Logistic(),xb), x*β), β)  \nend\n\nfunction delta_method(g, θ, Ω)\n  dG = ForwardDiff.jacobian(θ->g(θ),θ)\n  dG*Ω*dG'  \nend\n\nnfx = 100\nxmfx = zeros(nfx,3)\nxmfx[:,1] .= -3.0:(6.0/(nfx-1)):3.0\n\nmfx = logit_mfx(βhat,xmfx)\nvmfx = delta_method(β->logit_mfx(β,xmfx)[:,1], βhat, avar.variance/n)\nsdfx = sqrt.(diag(vmfx))\n\nusing Plots\nPlots.gr()\nplot(xmfx[:,1],mfx[:,1],ribbon=quantile(Normal(),0.95)*sdfx,fillalpha=0.5,\n     xlabel=\"x[1]\", ylabel=\"dP(y=1|x)/dx[1]\", legend=false,\n     title=\"Marginal effect of x[1] when x[2:k]=0\")"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The same approach can be used to compute standard errors and\nconfidence regions for the results of more complicated counterfactual\nsimulations, as long as the associated simulations are smooth\nfunctions of the parameters. However, sometimes it might be more\nnatural to write simulations with outcomes that are not smooth in the\nparameters. For example, the following code uses simulation to\ncalculate the change in the probability of $y$ from adding 0.1 to\n$x$."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "function counterfactual_sim(β, x, S)\n  function onesim()\n    e = rand(Logistic(), size(x)[1])\n    baseline= (x*β .+ e .> 0)\n    counterfactual= ((x.+0.1)*β .+ e .> 0)\n    mean(counterfactual.-baseline)\n  end\n  mean([onesim() for s in 1:S])\nend\nForwardDiff.gradient(β->counterfactual_sim(β,x,10),βhat)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, the gradient is 0 because the simulation function is a\nstep-function. In this situation, an alternative to the delta method\nis the simulation based approach of @krinsky1986. The procedure is\nquite simple. Suppose \n$\\sqrt{n}(\\hat{\\theta} - \\theta_0) \\indist N(0,\\Omega)$,\nand you want to an estimate of the distribution of $g(\\theta)$. \nRepeatedly draw $\\theta_s \\sim N(\\hat{\\theta}, \\Omega/n)$ and compute\n$g(\\theta_s)$. Use the distribution of $g(\\theta_s)$ for\ninference. For example, a 90% confidence interval for $g(\\theta)$\nwould be the 5%-tile of $g(\\theta_s)$ to the 95%-tile of\n$g(\\theta_s)$."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "Ω = avar.variance/n\nΩ = (Ω+Ω')/2         # otherwise, it's not exactly symmetric due to\n                     # floating point roundoff\nfunction kr_confint(g, θ, Ω, simulations; coverage=0.9)\n  θs = [g(rand(MultivariateNormal(θ,Ω))) for s in 1:simulations]\n  quantile(θs, [(1.0-coverage)/2, coverage + (1.0-coverage)/2])\nend\n\n@show kr_confint(β->counterfactual_sim(β,x,10), βhat, Ω, 1000)\n\n# a delta method based confidence interval for the same thing\nfunction counterfactual_calc(β, x)\n  baseline      = cdf.(Logistic(), x*β)\n  counterfactual= cdf.(Logistic(), (x.+0.1)*β)\n  return([mean(counterfactual.-baseline)])\nend\nv = delta_method(β->counterfactual_calc(β,x), βhat, Ω)\nghat = counterfactual_calc(βhat,x)\n@show [ghat + sqrt(v)*quantile(Normal(),0.05), ghat +\n       sqrt(v)*quantile(Normal(),0.95)]"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Alternative approaches to inference\n\n## Identification robust inference\n\nAs discussed in section 9 of @newey1994, there are three classic types\nof statistics for testing restrictions on parameters. Suppose you want\nto test $H_0: a(\\theta) = 0$. Let $\\hat{\\theta}$ denote the\nunrestricted estimate, and let $\\hat{\\theta}^r$ denote the\nestimate of $\\theta$ subject to the restriction. Wald test-statistics\nare based on $\\hat{\\theta}-\\hat{\\theta}^r$. Lagrange multiplier tests\nlook at the distribution of the estimated Lagrange\nmultiplier. Likelihood ratio (aka distance metric in @newey1994) tests\nlook at $Q_n(\\hat{\\theta}) - Q_n(\\hat{\\theta}^r)$. If we consider\ntesting $H_0: \\theta = \\vartheta$ for some fixed $\\vartheta$, then the\nusual approach based on the asymptotic normality of $\\hat{\\theta}$\ndiscussed above is exactly the same as the Wald test of this\nrestriction. As discussed by @newey1994, under standard assumptions,\nall three testing approaches are asymptotically equivalent. However,\nthe tests can and will differ in finite samples. More importantly, in\nthe face of identification problems, Wald tests tend to break down,\nwhile Lagrange multiplier and likelihood ratio style tests can\ncontinue to work. \n\nBy identification robust, we mean an inference procedure that has\ncorrect size regardless of whether identification is strong, weak,\nor partial. In the asymptotic normality of extremum estimators theorem\nabove, non-strong identification will create problems for assumption\n3, in particular the assumption that the Hessian is non-singular. For\nthis section, we will focus on GMM estimators. Identification problems\nmost often arrive and have been studied in the context of GMM. Also,\nit is not difficult to transform other extremum estimators into GMM. \n\nFor a GMM objective function of the form:\n$$ [1/n \\sum_i g_i(\\theta)] W_n [1/n \\sum g_i(\\theta]$$, \nif we assume:\n1. $1/\\sqrt{n} \\sum_i g_i(\\theta_0) \\indist N(0,\\Sigma)$\n2. $1/n \\sum_i \\nabla g_i(\\theta) \\inprob E[\\nabla g(\\theta)]$, \n   $W_n \\inprob W$\n3. $(D'WD)$ is nonsingular.\nthen the above theorem for asymptotic normality of extremum\nestimators implies that \n$$\n\\sqrt{n}(\\hat{\\theta} - \\theta_0) \\indist N(0,\\Omega)\n$$\nwhere \n$$\n \\Omega= (D'WD)^{-1} (D' W \\Sigma W D) (D'WD)^{-1}.\n$$\nIf we additionally assume $W_n \\inprob \\Sigma^{-1}$, e.g. observations\nare independent and $W_n =\n\\widehat{Var}(g_i(\\theta))^{-1}$, then the asymptotic variance\nsimplifies to $(D' \\Sigma D)^{-1}$. \n\n### Anderson-Rubin test\n\nAs already stated, the assumption that $(D'WD)$ is nonsingular is\nproblematic if we want to allow for identification problems. However,\nif we assume only that \n1. $1/\\sqrt{n} \\sum_i g_i(\\theta_0) \\indist N(0,\\Sigma)$\n2. $W_n \\inprob \\Sigma^{-1}$\nthen \n$$\nn [1/n \\sum g_i(\\theta_0)]' W_n^{-1} [1/n \\sum g_i(\\theta_0)]\n  \\indist \\chi^2_m\n$$\nwhere $m$ is the number of moments (dimension of $g_i(\\theta)$). This\nis called the Anderson-Rubin test. Note that this result holds without\nany explicit nonsingular assumption about a Hessian. Hence, there is\nhope that this result would be true even with identification\nproblems. Indeed, it is. @stock2000 first proposed using this test\nstatistic for weakly identified GMM estimators. @stock2002 gives an\noverview of this test and related tests with a focus on linear IV. \n\nTypical usage of the AR test is to invert the test to construct a\nconfidence region for $\\theta$. For each $\\theta \\in \\Theta$, \nlet \n$$\nAR(\\theta) = n [1/n \\sum g_i(\\theta)]' \\widehat{Var}(g_i(\\theta))^{-1}\n[1/n \\sum g_i(\\theta)] \n$$\nand let $c_{\\alpha}= \\alpha$ quantile of $\\chi^2_m$. Then a $\\alpha$\nconfidence region for $\\theta_0$ is\n$$\n\\{ \\theta \\in \\Theta: AR(\\theta) \\leq c_\\alpha \\}\n$$"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "function simulate_ivshare(n,β,π,ρ)\n  z = randn(n, size(π)[1])\n  endo = randn(n, length(β))\n  x = z*π .+ endo \n  e = rand(Normal(0,sqrt((1-ρ^2))),n).+endo[:,1]*ρ\n  y = cdf.(Logistic(), x*β .+ e)\n  return((y=y,x=x,z=z))  \nend\nn = 500\nk = 2\niv = 4\nβ0 = ones(k)\nπ0 = ones(iv,k)\nρ = 0.5\n(y,x,z) = simulate_ivshare(n,β0,π0,ρ)\n\nfunction gi_ivshare(β,y,x,z)\n  e = quantile.(Logistic(),y) .- x*β\n  e.*z\nend\n\nfunction gmmObj(θ,gi,W)\n  g = gi(θ)\n  m = mean(g,dims=1)\n  (size(g)[1]*( m*W*m')[1]) # return scalar, not 1x1 array\nend\nfunction ar(θ,gi)\n  gmmObj(θ,gi,inv(cov(gi(θ))))\nend\n\noptres = optimize(θ->gmmObj(θ,β->gi_ivshare(β,y,x,z),I),\n                  zeros(k), NewtonTrustRegion(), autodiff =:forward)\n@show β1 = optres.minimizer \noptres = optimize(θ->gmmObj(θ,β->gi_ivshare(β,y,x,z),inv(cov(gi_ivshare(β1,y,x,z)))),\n                  zeros(k), NewtonTrustRegion(), autodiff =:forward)\n@show βeff = optres.minimizer \noptres = optimize(θ->ar(θ,β->gi_ivshare(β,y,x,z)),\n                  zeros(k), NewtonTrustRegion(), autodiff =:forward)\n@show βcue = optres.minimizer"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "TO BE CONTINUED"
      ],
      "metadata": {}
    }
  ],
  "nbformat_minor": 2,
  "metadata": {
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia",
      "version": "1.0.1"
    },
    "kernelspec": {
      "name": "julia-1.0",
      "display_name": "Julia 1.0.1",
      "language": "julia"
    }
  },
  "nbformat": 4
}
