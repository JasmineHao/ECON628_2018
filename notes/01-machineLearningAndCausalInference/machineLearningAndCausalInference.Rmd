---
title       : "Machine Learning and causal inference"
subtitle    : 
author      : Paul Schrimpf
job         : 
date        : "`r format(Sys.time(), '%d %B, %Y')`"
bibliography: "ml.bib"
output      : 
    html_document : 
        toc : true
        toc_depth : 2
        toc_float : true
        number_sections : true
        theme : journal
        css : ../628notes.css
    revealjs::revealjs_presentation:
        self_contained: false
        theme: league
        transition: slide
        center : true
        highlight : zenburn
        reveal_plugins: ["chalkboard","zoom","notes"]
        reveal_options:
            slideNumber: false
            help : true
            previewLinks: true
            chalkboard:
                theme: whiteboard
                toggleNotesButton: true
                toggleChalkboardButton: true
        revealjs_url : "http://faculty.arts.ubc.ca/pschrimpf/526/optimalControl_files/reveal.js-3.3.0.1"
## To create an html file from this, in R enter'source("../myrender.R"); myrender("filename.Rmd")'
--- 

---

# Introduction 

<aside class="notes">
    These notes will examine the incorportion of machine learning
    methods in classic econometric techniques for estimating causal
    effects. More specifally, we will focus on estimating treatment
    effects using matching and instrumental variables. In these
    estimators (and many others) there is a low-dimensional parameter
    of interest, such as the average treatment effect, but estimating
    it requires also estimating a potentially high dimensional
    nuisance parameter, such as the propensity score. Machine learning
    methods were developed for prediction with high dimensional
    data. It is then natural to try to use machine learning for
    estimating high dimensional nuisance parameters. Care must be
    taken when doing so though because the flexibility and complexity
    that make machine learning so good at prediction also pose
    challenges for inference. 
</aside>
---

<aside class="notes">
    To illustrate, 
</aside>

## Example: IV

- 

---

## Example: Matching

- 

---


## References

- Matching 
    - **@imbens2015** 
    - @imbens2004 

- Surveys on machine learning in econometrics
    - **@athey2017**
    - @mullainathan2017
    - @athey2017b
    - @athey2015
    
- Machine learning 
    - @breiman2001 
    - @friedman2008
    
- Introduction to lasso 
    - @belloni2011
    - @friedman2008 section 3.4
    
- Introduction to random forests
    - @friedman2008 section 9.2

<aside class="notes">
    **Bold** references are recommended reading.  They are generally
    shorter and less technical than some of the others. Aspiring
    econometricians should read much more than just the bold
    references. 
</aside>
---

- Neyman orthogonalization 
    - **@chernozhukov2017**
    - @chernozhukov2015
    - @chernozhukov2016
    - @belloni2017

- Lasso for causal inference
    - **@belloni2014jep** 
    - @belloni2012
    - @belloni2014
    - @chernozhukov2016b
    - @hdm hdm R package 
    
- Random forests for causal inference
    - @athey2016
    - @wager2018
    - @grf grf R package
    - @athey2016b

<aside class="notes">
    There is considerable overlap among these categories. The papers
    listed under Neyman orthogonalization all include use of Lasso and
    some include random forests. 
</aside>
---

# Matching

---

<!-- ---------------------------------------------------------------------- -->

# Lasso 

---

## Lasso

---

## Example showing good for prediction

---

## Partially linear model

---

## ATE with unconfoundedness

- role of orthogonality

---

## IV with Lasso

---

<!-- ---------------------------------------------------------------------- -->

# Random forests

---

## Random forests

--- 

## Example showing good for prediction

---

## ATE with random forests

- @athey2016, @wager2018, etc
---

## IV and LATE with random forests

- @athey2016, @wager2018, etc

<!-- ---------------------------------------------------------------------- -->

# Double debiased machine learning

- @chernozhukov2016, @chernozhukov2017


# Bibliography

---
