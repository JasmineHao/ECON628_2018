{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\ntitle       : \"Bootstrap\"\nsubtitle    :\nauthor      : Paul Schrimpf\ndate        : `j using Dates; print(Dates.today())`\nbibliography: \"bs.bib\"\n---\n\n<a rel=\"license\"\nhref=\"http://creativecommons.org/licenses/by-sa/4.0/\"><img alt=\"Creative\nCommons License\" style=\"border-width:0\"\nsrc=\"https://i.creativecommons.org/l/by-sa/4.0/88x31.png\"\n/></a><br />This work is licensed under a <a rel=\"license\"\nhref=\"http://creativecommons.org/licenses/by-sa/4.0/\">Creative\nCommons Attribution-ShareAlike 4.0 International License</a>.\n\n### About this document {-}\n\nThis document was created using Weave.jl. The code is available in\n[the course github\nrepository](https://github.com/ubcecon/ECON628_2018/paul). The same\ndocument generates both static webpages and associated jupyter\nnotebooks.\n\n$$\n\\def\\indep{\\perp\\!\\!\\!\\perp}\n\\def\\Er{\\mathrm{E}}\n\\def\\R{\\mathbb{R}}\n\\def\\En{{\\mathbb{E}_n}}\n\\def\\Pr{\\mathrm{P}}\n\\newcommand{\\norm}[1]{\\left\\Vert {#1} \\right\\Vert}\n\\newcommand{\\abs}[1]{\\left\\vert {#1} \\right\\vert}\n\\DeclareMathOperator*{\\argmax}{arg\\,max}\n\\DeclareMathOperator*{\\argmin}{arg\\,min}\n\\def\\inprob{\\,{\\buildrel p \\over \\rightarrow}\\,} \n\\def\\indist{\\,{\\buildrel d \\over \\rightarrow}\\,} \n$$\n\n# Introduction\n\nThe bootstrap is a method of inference that utilizes resampling. The\nbasic idea is as follows. Suppose you have some parameter of interest\nfor which you want to do inference. Let $T_n$ denote some test\nstatistic involving the estimator. The test \nstatistic is a function of data, so the distribution of the estimator\nis a function of the distribution of data. Let $F_0$\ndenote the exact, finite sample distribution of the data. Let \n$G_n(\\tau, F_0) = \\Pr(\\hat{\\theta}_n \\leq \\tau$ denote the\nexact finite sample distribution of the statistic. To do inference, we\nwould like to know $G_n(\\tau, F_0)$. This is generally impossible\nwithout strong assumptions. Asymptotics get around this problem by\napproximating $G_n(\\tau, F_0)$ with its asymptotic distribution,\n$G_\\infty(\\tau,F_0)$. The bootstrap is an alternative approach (but\nthe formal justification for the bootstrap still relies on\nasymptotics). The bootstrap approximates $G_n(\\tau, F_0)$ by replacing\n$F_0$ with an estimate, $\\hat{F}_n$. One common estimate of\n$\\hat{F}_n$ is simply the empirical CDF. When observations are\nindependent, we can randomly draw $T^*_n$ from \n$G_n(\\tau, \\hat{F}_n)$ by randomly drawing with replacement a sample\nof size $n$ from the orgininal observations, and then computing\n$T^*_n$ for this sample. We can do this repeatedly, and use\nthe distribution of the resulting $\\hat{\\theta}^*_n$'s to calculate\n$G_n(\\tau,\\hat{F}_n)$. \n\n\nAs a quick example, here's some code where the statistic is the sample\nmedian minus its true value."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using StatPlots, Distributions\ndgp(n) = rand(n).^2\nestimator(x) = median(x)\n# simulating T ~ G_n(τ,F_0)\nn = 200\nS = 1000\nT = [estimator(dgp(n)) for s in 1:S] .- 0.25\nfunction bootstrap(data, estimator, S)\n  n = length(data)\n  dist = Categorical(fill(1.0/n,n))\n  θhat = estimator(data)\n  T = [estimator(data[[rand(dist) for i in 1:n]]) for s in 1:S] .- θhat \nend\nTboot = bootstrap(dgp(n),estimator, S)\nPlots.pyplot()\ndensity(T, label=\"true distribution\")\ndensity!(Tboot, label=\"bootstrap distribution\")"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n\n@mackinnon2006 and @mackinnon2009 are good practical introductions to\nthe bootstrap. @horowitz2001 is also a good overview, and includes\nmore precise statements of theoretical results, but does not contain\nproofs. @gine1997 is a rigorous and fairly self-contained\ntheoretical treatment of the bootstrap. Although the bootstrap works\nin many situations, it does not always work. For example, @abadie2008\nshow the failure of the bootstrap for matching estimators. See\n@andrews2000, @andrews2009, and @romano2012 for theoretical\ndevelopments on situations where the bootstrap fails.  @hall1994 gives\na theoretical overview of when the bootstrap provides asymptotic\nrefinement. @chernozhukov2017 discusses the bootstrap in high\ndimensional models.\n\n# Theory\n\nThis section follows the approach @van2000. We focus on the case where\n$T_n = \\frac{\\hat{\\theta}_n - \\theta_0}{\\hat{\\sigma}_n}$ is a\nt-statistic. A simple and useful result is that if $T_n$ and $T^*_n$\nboth converge to the same distribution, then the bootstrap is\nconsistent. \n\n<div class=\"theorem\">\n*Suppose that \n$$\nT_n = \\frac{\\hat{\\theta}_n - \\theta_0}{\\hat{\\sigma}_n} \\leadsto T\n$$\nand \n$$\nT_n^* =  \\frac{\\hat{\\theta}^*_n - \\hat{\\theta}_n}{\\hat{\\sigma}^*_n} \\leadsto T\n$$\nconditional on the data, for some random variable $T$ with a\ncontinuous distribution function. Then \n$$\n| G_n(\\tau, F_0) - G_n(\\tau,\\hat{F}_n) | \\inprob 0\n$$\nand in particular,\n$$\n\\Pr(\\theta_0 \\in [\\hat{\\theta}_n - G_n^{-1}(\\alpha/2, \\hat{F}_n)\n\\hat{\\sigma}_n ,  \\hat{\\theta}_n - G_n^{-1}(1-\\alpha/2, \\hat{F}_n)\n\\hat{\\sigma}_n ]) \\to 1-\\alpha.\n$$\n*\n</div>\n\n*Proof sketch:* $T_n$ and $T^*_n$ both $\\leadsto T$ immediately\nimplies $G_n(\\tau, F_0) \\inprob G_\\infty(\\tau)$ and\n$G_n(\\tau,\\hat{F}_n) \\inprob G_\\infty(\\tau)$, where $G_\\infty(\\tau)$\nis the CDF of $T$. This implies that \n$G^{-1}_n(\\tau,\\hat{F}_n) \\inprob G^{-1}_\\infty(\\tau)$ for all $\\tau$\nwhere $G_\\infty$ is continuous. Then we have\n$$\n\\Pr(\\theta_0 \\geq \\hat{\\theta}_n - G_n^{-1}(\\tau, \\hat{F}_n)\n \\hat{\\sigma}_n) = \\Pr(\\frac{\\theta_0 -\n \\hat{\\theta}_n}{\\hat{\\sigma}_n} \\leq G_n^{-1}(\\tau, \\hat{F}_n) \\to\n \\Pr(T \\leq G^{-1}_\\infty(\\tau)) = \\tau.\n$$\n\nThis theorem is very simple, but it is useful because it suggest a\nsimple path to showing the consistency of the bootstrap: simply show\nthat $T_n^*$ has the same asymptotic distribution as $T_n$. Here is a\nsimple result for when $T_n^*$ is constructed by sampling with\nreplacement from the empirical distribution. We will let\n$\\mathbb{P}_n$ denote the empirical distribution, and $x_i^*$ denote\ndraws of $x_i$ from it.\n\n<div class=\"theorem>\n*Let $x_1, x_2, ...$ be i.i.d. with mean $\\mu$ and variance\n$\\sigma^2$. Then conditional on $x_1, ...$ for almost every sequence\n$$\n\\sqrt{n} (\\bar{x}_n^* - \\bar{x}_n) \\indist N(0,\\sigma^2)\n$$\n*\n</div>\n\n**Proof sketch** it is straightforward to show that $\\Er[x_i^* |\n\\mathbb{P}_n] = \\bar{x}_n$ and $Var(x_i^*|\\mathbb{P}_n) = \\bar{x^2}_n\n- \\bar{x}_n^2 \\to \\sigma^2$. Applying the Lindeberg CLT then gives the\nresult.\n\n\n## Pivotal statistics\n\nThe above results imply that the bootstrap works for both \n$S_n = \\sqrt{n}(\\bar{x}_n - \\mu_0)$ and \"studentized\" a statistic \n$T_n = \\sqrt{n}(\\bar{x}_n - \\mu_0)/\\hat{\\sigma}_n$. There is some\nadvantage to using the later. A statistic is called pivotal if its\ndistribution is completely known. If we assume $x_i \\sim N$, then\n$T_n$ is pivotal and has a t-distribution. If we aren't willing to\nassume normality, then the distribution of $T_n$ is unknown, but its\nasymptotic distribution is completely known, $N(0,1)$. Such a\nstatistic is called asymptotically pivotal. $S_n$ is not\nasymptotically pivotal because its asymptotic distribution depends on\nthe unknown variance. It is possible to show that the bootstrap\ndistribution of asymptotically pivotal statistics converge faster than\neither the usual asymptotic approximation or the bootstrap distribution of\nnon-pivotal statistics. See @hall1994 for details. \n\nHere is a simulation to illustrate."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "dgp(n) = rand(n)\nestimator(x) = mean(x)\nθ0 = 0.5\nN = 5:5:20\nB = 999\nfunction simulatetests(n)\n  function bootstrap(data, stat, S)\n    n = length(data)\n    dist = Categorical(fill(1.0/n,n))\n    T = [stat(data[[rand(dist) for i in 1:n]]) for s in 1:S] \n  end\n  data = dgp(n)\n  t = sqrt(n)*(mean(data)-θ0)/std(data)\n  [cdf(Normal(),t),   \n   mean(t.<bootstrap(data, d->(sqrt(n)*(mean(d) - mean(data))/std(d)), B)),\n   mean((mean(data)-θ0) .< bootstrap(data, d->(mean(d)-mean(data)), B))]\nend\nres=[hcat([simulatetests.(n) for s in 1:1000]...) for n in N]\n\np = 0:0.01:1\ni = 1\nplot(p, p->(mean(res[i][1,:].<p)-p), title=\"N=$(N[i])\",  label=\"asymptotic\")\nplot!(p, p->(mean(res[i][2,:].<p)-p), title=\"N=$(N[i])\", label=\"pivotal bootstrap\")\nplot!(p, p->(mean(res[i][3,:].<p)-p), title=\"N=$(N[i])\", label=\"non-pivotal boostrap\")\n\ni = 4\nplot(p, p->(mean(res[i][1,:].<p)-p), title=\"N=$(N[i])\",  label=\"asymptotic\")\nplot!(p, p->(mean(res[i][2,:].<p)-p), title=\"N=$(N[i])\", label=\"pivotal bootstrap\")\nplot!(p, p->(mean(res[i][3,:].<p)-p), title=\"N=$(N[i])\", label=\"non-pivotal boostrap\")"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this simulation design it appears impossible to see the difference\nin performance. Theoretically, the difference in convergence rates is\nsomething like $n^{-1/2}$ vs $n^{-1/3}$, so it we really do need to\nhave the right simulation design and method of visualizing the results\nto see the difference."
      ],
      "metadata": {}
    }
  ],
  "nbformat_minor": 2,
  "metadata": {
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia",
      "version": "1.0.2"
    },
    "kernelspec": {
      "name": "julia-1.0",
      "display_name": "Julia 1.0.2",
      "language": "julia"
    }
  },
  "nbformat": 4
}
