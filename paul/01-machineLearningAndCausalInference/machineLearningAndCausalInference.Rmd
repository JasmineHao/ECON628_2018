---
title       : "Machine Learning and causal inference"
subtitle    : 
author      : Paul Schrimpf
job         : 
date        : "`r format(Sys.time(), '%d %B, %Y')`"
bibliography: "ml.bib"
output      : 
    html_document : 
        toc : true
        toc_depth : 2
        toc_float : true
        number_sections : true
        theme : journal
        css : ../628notes.css
    revealjs::revealjs_presentation:
        self_contained: false
        theme: league
        transition: slide
        center : true
        highlight : zenburn
        reveal_plugins: ["chalkboard","zoom","notes"]
        reveal_options:
            slideNumber: false
            help : true
            previewLinks: true
            chalkboard:
                theme: whiteboard
                toggleNotesButton: true
                toggleChalkboardButton: true
## To create an html file from this, in R enter'source("../renderAll.R"); renderAll("filename.Rmd")'
--- 

---

# Introduction 

<aside class="notes">
    These notes will examine the incorportion of machine learning
    methods in classic econometric techniques for estimating causal
    effects. More specifally, we will focus on estimating treatment
    effects using matching and instrumental variables. In these
    estimators (and many others) there is a low-dimensional parameter
    of interest, such as the average treatment effect, but estimating
    it requires also estimating a potentially high dimensional
    nuisance parameter, such as the propensity score. Machine learning
    methods were developed for prediction with high dimensional
    data. It is then natural to try to use machine learning for
    estimating high dimensional nuisance parameters. Care must be
    taken when doing so though because the flexibility and complexity
    that make machine learning so good at prediction also pose
    challenges for inference. 
</aside>

---

## Example: partially linear model

$$
    y_i = \alpha d_i + f(x_i) + \epsilon_i 
$$

- Interested in $\alpha$ 
- Assume $E[\epsilon|d,x] = 0$
- Nuisance parameter $f()$

<aside class="notes"> 
    The simplest example of the setting we will analyze is a partially
    linear model. We have some regressor of interest, $d$, and we want
    to estimate the effect of $d$ on $y$. We have a rich enough set of
    controls that we are willing to believe that $E[\epsilon|d,x] =
    0$. $d_i$ and $y_i$ are scalars, while $x_i$ is a vector. We are
    not interested in $x$ per se, but we need to include them to avoid
    omitted variable bias.
    
    Typical applied econometric practice would
    be to choose some transfrom of $x$, say $X = T(x)$, where $X$
    could be some subset of $x$, along with interactions, powers, and
    so on. Then estimate a linear regression 
    $$
     y = \alpha d + X'\beta + \epsilon
    $$ 
    and then perhaps also report results for a handful of different
    choices of $T(x)$. 
    
    Some downsides to the typical applied econometric practice
    include:
    
    - The choice of T is arbitrary, which opens the door to specification
      searching and p-hacking.
      
    - If $x$ is high dimensional, and $X$ is low dimensional, a poor
      choice will lead to omitted variable bias. Even if $x$ is low
      dimensional, if $f(x)$ is poorly approximated by $X'\beta$,
      there will be omitted variable bias.
    
    In some sense, machine learning can be thought of as a way to
    choose $T$ is an automated and data-driven way. There will be
    still be a choice of machine learning method and often tuning
    parameters for that method, so some arbitrary decisions
    remain. Hopefully though these decisions have less impact. 
    
    You may already be familiar with traditional nonparametric
    econometric methods like series / sieves and kernels. These share
    much in common with machine learning. What makes
    machine learning different that traditional nonparametric methods? 
    Machine learning methods appear to have better predictive
    performance, and arguably more practical data-driven methods to choose
    tuning parameters. Machine learning methods can deal with high
    dimensional $x$, while traditional nonparametric methods focus on
    situations with low dimensional $x$. 
</aside> 
        
---

## Example: Matching

$\def\indep{\perp\!\!\!\perp}$

- Binary treatment $d_i \in \{0,1\}$
- Potential outcomes $y_i(0), y_i(1)$, observe $y_i = y_i(d_i)$
- Interested in average treatment effect : $\alpha = E[y_i(1) -
  y_i(0)]$
- Covariates $x_i$
- Assume unconfoundedness : $d \indep y_i(1), y_i(0) | x_i$


<aside class="notes"> 
    The partially linear and matching models are
    closely related. If the conditional mean independence assumption
    of the partially linear model is strengthing to conditional
    indepence then the partially linear model is a special case of the
    matching model with constant treatment effects, 
    $y_i(1) - y_i(0) = \alpha$. 
    Thus the matching model can be viewed as a generalization
    of the partially linear model that allows for treatment effect
    heterogeneity.     
</aside>

---

## Example: Matching

- Estimatable formulae for ATE : 
$$
\begin{align*}
  \alpha = & E\left[\frac{y_i d_i}{P(d = 1 | x_i)} + \frac{y_i
      (1-d_i)}{1-P(d=1|x_i)} \right] \\
  \alpha = & E\left[E[y_i | d_i = 1, x_i] - E[y_i | d_i = 0 , x_i]\right] \\
  \alpha = & E\left[d_i \frac{y_i - E[y_i | d_i = 1,
      x_i]}{P(d=1|x_i)} + (1-d_i)\frac{y_i - E[y_i | d_i = 0,
      x_i]}{1-P(d=1|x_i)} + E[y_i | d_i = 1, x_i] - E[y_i | d_i = 0 ,
      x_i]\right] 
\end{align*}
$$

<aside class = "notes"> 
    All the expectations in these three formulae involve observable
    data. Thus, we can form an estimate of $\alpha$ be replacing the
    expectations and conditional expectations with appropriate
    estimators. For example, to use the first formula, we could
    estimate a logit model for the probability of treatment, 
    $$
        \hat{P}(d=1|x_i) = \frac{e^{X_i' \hat{\beta}}}{1+e^{X_i'\hat{\beta}}}
    $$    
    where, as above, $X$ is a some chosen transformation of
    $x_i$. Then we simply take an average to estimate $\alpha$.
    $$
    \hat{\alpha} = \frac{1}{n} \sum_{i=1}^n \frac{y_i d_i}{\hat{P}(d=1|x_i)} + 
    \frac{y_i(1-d_i)} {1-\hat{P}(d=1|x_i)}        
    $$
    As in the partially linear model, estimating the parameter of
    interest, $\alpha$, requires estimating a potentially high
    dimensional nuisance parameter, in this case
    $\hat{P}(d=1|x)$. Similarly, the second expression would require
    estimating conditional expectations of $y$ as nuisance
    parameters. The third expression requires estimating both
    conditional expecations of $y$ and $d$. 
    
    The third expression might appear needlessly complicated, but we
    will see later that it has some desirable properties that will
    make using it essential when very flexible machine learning
    estimators for the conditional expectations are used. 
    
    TODO: add empirical examples of heart catheterization from
    @athey2017 and abortion & crime from @donahueXXX as reanalyzed by
    @chernozhukov2014. 
    
    TODO? discuss why it's called matching.        
</aside>

---

<aside class = "notes">
    Both the partially linear model and treatment effects model can be
    extended to situations with endogeneity and instrumental
    variables. 
</aside>

## Example: IV

- 

---

## References

- Matching 
    - **@imbens2015** 
    - @imbens2004 

- Surveys on machine learning in econometrics
    - **@athey2017**
    - @mullainathan2017
    - @athey2017b
    - @athey2015
    
- Machine learning 
    - @breiman2001 
    - @friedman2008
    - @james2013
    
- Introduction to lasso 
    - @belloni2011
    - @friedman2008 section 3.4
    
- Introduction to random forests
    - @friedman2008 section 9.2

<aside class="notes">
    **Bold** references are recommended reading.  They are generally
    shorter and less technical than some of the others. Aspiring
    econometricians should read much more than just the bold
    references. 
</aside>

---

- Neyman orthogonalization 
    - **@chernozhukov2017**
    - @chernozhukov2015
    - @chernozhukov2016
    - @belloni2017

- Lasso for causal inference
    - **@belloni2014jep** 
    - @belloni2012
    - @belloni2014
    - @chernozhukov2016b
    - @hdm hdm R package 
    
- Random forests for causal inference
    - @athey2016
    - @wager2018
    - @grf grf R package
    - @athey2016b

<aside class="notes">
    There is considerable overlap among these categories. The papers
    listed under Neyman orthogonalization all include use of Lasso and
    some include random forests. 
</aside>

---

<!-- ---------------------------------------------------------------------- -->

# Introduction to machine learning 

---

## Predicting pipeline revenues

- Machine learning is tailored for prediction, let's look at some data
  to find out
- Data on US natural gas pipelines 
   - Combination of FERC Form 2, EIA Form 176, and other sources,
     compiled by me 
   - 1996-2016, 236 pipeline companies, 1219 company-year observations
- Predict: $y =$ profits from transmission of natural gas
- Covariates: year, capital, discovered gas reserves, well head gas price,
     city gate gas price, heating degree days, state(s) that each
     pipeline operates in

<aside class="notes">
    This is a dataset that I put together for a current research
    project. I'm happy to share it upon request, but it is not ready to
    be posted publicly yet. 
</aside>
---

```{r, loadpipeline, results='show', warning=FALSE, cache=TRUE}
load("~/natural-gas-pipelines/dataAndCode/pipelines.Rdata")
# data has problems before 1996 due to format change
data <- subset(data,report_yr>=1996)
# replace NA state weights with 0's
data[,59:107][is.na(data[,59:107])] <- 0
# spaces in variable names will create problems later
names(data) <- gsub(" ",".",names(data))
summary(data[,c("transProfit","transPlant_bal_beg_yr","cityPrice","wellPrice")])
```

---

```{r, pipeline-figure, cache=TRUE, results='hide', warning=FALSE,message=FALSE}
library(GGally)
ggpairs(data[,c("transProfit","transPlant_bal_beg_yr","cityPrice","wellPrice")],
        lower=list(continuous="smooth"))  + theme_minimal()
```

<aside class="notes">    
    Anytime you start working with a new dataset, you should look at
    some summary statistics and exploratory figures. 
</aside>

---

## Predicting pipeline revenues : methods

- OLS : 67 covariates (year dummies and state(s) create a lot)
- Lasso 
- Random forests 
- Randomly choose 75% of sample to fit the models, then look at
  prediction accuracy in remaining 25%

<aside class="notes">
    We are focusing on Lasso and random forests because these are the
    two methods that econometricians have worked on the most. Other
    methods such as neural nets and support vector machines are also
    worth exploring. For now, you can think of Lasso and random
    forests these as black boxes that generate predictions from
    data. We will go into more detail soon.
</aside>
--- 

```{r, pipeline-predict, cache=TRUE}
## Create X matrix for OLS and random forests
xnames <-c("transPlant_bal_beg_yr",   "reserve",   "wellPrice",  "cityPrice",
           "plantArea",  "heatDegDays", 
           names(data)[59:107] )
yname <- "transProfit"
fmla <- paste(yname,"~",paste(xnames,collapse=" + "),"+ as.factor(report_yr)")
ols <- lm(fmla,data=data,x=TRUE,y=TRUE)
X <- ols$x[,!(colnames(ols$x) %in% c("(Intercept)")) &
            !is.na(ols$coefficients)]
print(dim(X))
y <- ols$y
train <- runif(nrow(X))<0.75

# OLS prediction on training set
ols <- lm(y ~ X)
y.hat.ols <- ols$coefficients[1] + X %*% ols$coefficients[2:(length(ols$coef))]
df <- data.frame(y=y, y.hat=y.hat.ols, train=train, method="ols")

## Lasso
library(glmnet)
# Create larger X matrix for lasso
fmla.l <- paste(yname,"~ (",
                paste(xnames,collapse=" + "),")*(report_yr + transPlant_bal_beg_yr +
    reserve + wellPrice + cityPrice + plantArea + heatDegDays) + ",
                paste(sprintf("I(%s^2)",xnames[1:6],collapse=" + "))
                )
reg <- lm(fmla.l, data=data, x=TRUE,y=TRUE)
Xl <- reg$x[,!(colnames(reg$x) %in% c("(Intercept)")) &
             !is.na(reg$coefficients)]
print(dim(Xl))
lasso <- cv.glmnet(Xl[train,],y[train],alpha=1,parallel=FALSE,
                   standardize=TRUE, intercept=TRUE, nfolds = 50)
y.hat.lasso <- predict(lasso, Xl, s=lasso$lambda.1se, type="response")
df <- rbind(df,  data.frame(y=y, y.hat=as.vector(y.hat.lasso),
                            train=train, method="lasso"))

## Random forest
library(grf)
rf <- regression_forest(X[train,],y[train],tune.parameters = TRUE)
y.hat.rf  <-  predict(rf, X)$predictions
df <- rbind(df, data.frame(y=y, y.hat=y.hat.rf, train=train,
                           method="random forest"))
```

---

```{r, pipelinePredPlot, cache=TRUE, echo=FALSE}
ggplot(data=df,aes(x=y,y=y.hat,colour=method,shape=train)) +  geom_point(alpha=0.5) +
  geom_line(aes(y=y)) + theme_minimal()
```

---

```{r, pipelinePredSummary, echo=FALSE, cache=TRUE, results='as-is'}
library(kableExtra)
fn <- function(df)  with(df,c(mean((y.hat - y)^2)/var(y),
                              mean(abs(y.hat - y))/mean(abs(y-mean(y)))))
tab1 <-unlist(by(subset(df,train), df$method[train],  FUN=fn))
tab1 <- (matrix(tab1,nrow=2))
rownames(tab1) <- c("relative MSE","relative MAE")
colnames(tab1) <- c("OLS","Lasso","Random forest")
tab2 <- unlist(by(subset(df,!train), df$method[!train],  FUN=fn))
tab2 <- (matrix(tab2,nrow=2))
rownames(tab2) <- c("relative MSE","relative MAE")
colnames(tab2) <- c("OLS","Lasso","Random forest")

kable_styling(kable(tab1, caption="Training sample", format="html"),
              bootstrap_options = c("striped", "hover", "condensed",
              "responsive"), full_width=F)
kable_styling(kable(tab2, caption="Evaluation sample", format="html"),
              bootstrap_options = c("striped", "hover", "condensed",
              "responsive"), full_width=F)
```
---

## Lasso

---

## Example showing good for prediction

---

## Partially linear model

---

## ATE with unconfoundedness

- role of orthogonality

---

## IV with Lasso

---

<!-- ---------------------------------------------------------------------- -->

# Random forests

---

## Random forests

--- 

## Example showing good for prediction

---

## ATE with random forests

- @athey2016, @wager2018, etc
---

## IV and LATE with random forests

- @athey2016, @wager2018, etc

<!-- ---------------------------------------------------------------------- -->

# Double debiased machine learning

- @chernozhukov2016, @chernozhukov2017


# Bibliography

---
